{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7df9670",
   "metadata": {},
   "source": [
    "# Explaining Distribution Shifts in Time using Random Forests and t-SNE on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4441cc5",
   "metadata": {},
   "source": [
    "In this Python notebook, we use machine learning techniques to explain a distribution shift in a time series. Specifically, we aim to identify which data points are specific to the time before or after the distribution shift, as well as which data points are distributed independently of time.\n",
    "\n",
    "To achieve this goal, we train a random forest (RF) model on the Fashion-MNIST dataset, which consists of images of fashion items. Fashion-MNIST is a classic dataset in machine learning that is often used as a benchmark for image classification and dimensionality reduciton tasks, as it presents a challenging high-dimensional classification problem with various classes to choose from. Its classes are not as easily separated as those of the classic MNIST digit dataset. The random forest model is a versatile and robust model that can handle a wide range of input data and is relatively insensitive to weak data assumptions making it a common choice in stream learning. The model is well-suited to handle the complex and high-dimensional input data, making it a good choice for this machine learning task.\n",
    "\n",
    "To visualize the results, we use t-SNE to project the high-dimensional data onto a two-dimensional plot. The t-SNE algorithm is used to derive an embedding space from the information that is extracted during the random forest training using ideas from discriminative dimensionality reduction. The plot shows how clusters change before and after the distribution shift, providing insights to better understand the nature of the shift. By visualizing the embedding space, we can see which data points are specific to the time before or after the distribution shift and how the clusters change over time. This information can be used to improve the performance of the random forest model and gain a better understanding of the data.\n",
    "\n",
    "Furthermore, we showcase the the observed change in the distribution using counterfactual explanation which are based on contrasting similar yet differently classified datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f39f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse.linalg import eigs\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6381766",
   "metadata": {},
   "source": [
    "## Exploring the Fashion-MNIST Dataset: Mean Images, Similarity Matrix, and t-SNE Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dddb73",
   "metadata": {},
   "source": [
    "In this part, we explore the Fashion-MNIST dataset using mean images, similarity, and t-SNE. We first compute the mean image for each digit class and use it to compute the similarity between each pair of digit classes. We then visualize the similarity matrix using matplotlib. Finally, we use t-SNE to project the high-dimensional data onto a two-dimensional space, which allows us to visualize the dataset and gain insights into its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading the dataset...')\n",
    "mnist = fetch_openml('Fashion-MNIST')#, parser='auto')\n",
    "print('Download complete.')\n",
    "\n",
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Store the dataset in an appropriate format\n",
    "X = np.array(mnist.data).reshape((-1, 28, 28))\n",
    "y = np.array(mnist.target).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e4896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty numpy array to store the mean images for each label\n",
    "Xmean = np.zeros((len(set(y)), 28, 28))\n",
    "\n",
    "# Iterate over each label in the dataset\n",
    "for i in set(y):\n",
    "    # Compute the mean image for the current label\n",
    "    Xmean[i] = X[y == i].mean(axis=0)\n",
    "    \n",
    "# Create a figure with two rows and five columns to plot class means\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 3))\n",
    "\n",
    "# Iterate over each subplot and plot the data\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        label = i * 5 + j\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].matshow(Xmean[label])\n",
    "        axs[i, j].set_title('{} mean:'.format(labels[label]))\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdc57f-96c3-4494-950c-84a1e6476d46",
   "metadata": {},
   "source": [
    "As can be seen some classes are quite similar when compared on a mean value level. Consider for example Pullover, Coat, and Shirt. This is a important difference when compared to the MNIST dataset where all digit classes are easily separated by thair mean values.\n",
    "\n",
    "We can observe a similar effect in the (pixel-wise) similarity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90925851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty numpy array to store the dot product between the mean images\n",
    "scalar = np.zeros((Xmean.shape[0], Xmean.shape[0]))\n",
    "\n",
    "# Iterate over each pair of labels and compute the dot product between their mean images\n",
    "for i in range(Xmean.shape[0]):\n",
    "    for j in range(Xmean.shape[0]):\n",
    "        scalar[i, j] = Xmean[i].flatten() @ Xmean[j].flatten()\n",
    "\n",
    "# Normalize the similarity matrix so that its rows and columns sum to 1\n",
    "scalar = scalar / (scalar.sum(axis=0)[:, None] * scalar.sum(axis=1)[None, :])\n",
    "\n",
    "# Display the similarity matrix using matplotlib\n",
    "plt.matshow(scalar)\n",
    "plt.xlabel('Digit Class')\n",
    "plt.xticks(range(10))\n",
    "plt.ylabel('Digit Class')\n",
    "plt.yticks(range(10))\n",
    "plt.title('Similarity matrix between mean images')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51774ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 5000 samples from the dataset\n",
    "embedding_selection = np.random.choice(range(X.shape[0]), size=5000, replace=False)\n",
    "\n",
    "# Fit the t-SNE model to the selected samples and obtain the low-dimensional embeddings\n",
    "print('Fitting the t-SNE model...')\n",
    "tsne = TSNE(init='random', learning_rate='auto')\n",
    "X_embedded = tsne.fit_transform(X[embedding_selection].reshape(-1, 28*28))\n",
    "print('Fitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size to 10x10 inches\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Specify the percentage of samples to display for each label\n",
    "percentage = 30\n",
    "\n",
    "# Iterate over each label in the dataset\n",
    "for i in set(y):\n",
    "    # Select the samples with the current label\n",
    "    selection_label = y[embedding_selection] == i\n",
    "    # Calculate the number of samples to display based on the specified percentage\n",
    "    num_samples = int(np.ceil(np.sum(selection_label) * percentage / 100))\n",
    "    # Select a random subset of the samples with the current label\n",
    "    selection_random = np.random.choice(np.where(selection_label)[0], size=num_samples, replace=False)\n",
    "    # Create a scatter plot of the selected samples, with the label as the marker and alpha set to 0.5\n",
    "    #plt.scatter(X_embedded[selection_random, 0], X_embedded[selection_random, 1], marker=\"$%i$\" % i, alpha=0.5, s=25)\n",
    "    plt.scatter(X_embedded[selection_random, 0], X_embedded[selection_random, 1], marker=\"$%s$\" % labels[i][:4], alpha=0.5, s=250)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(f't-SNE visualization of MNIST ({percentage}% of the dataset)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871161d-e61e-4deb-bbdd-cf44cbd9f4a3",
   "metadata": {},
   "source": [
    "t-SNE is based on k-nearest neighbors embeddings. Thus, the neighborhoods are transferred from the high-dimensional space to the low-dimensional embedding. As can be seen in the scatter plot, there is a large overlap between the different classes when using a metric that is not specifically designed, i.e., the Eucliden metric, which fits our previous observation. This is again a major difference from the MNIST dataset, where the classes are mostly well separated in the t-SNE plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41b907",
   "metadata": {},
   "source": [
    "## Preparing the data for the distribution shift analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de0179",
   "metadata": {},
   "source": [
    "In this part, we prepare the Fashion-MNIST dataset for the distribution shift analysis. We map each digit in the dataset to a label indicating whether it occurs before or after the change point, or both, or neither. We then remove the samples with label 0 (i.e., pieces that do not occur before nor after the change point) from the input features and labels. Finally, we randomly assign labels of 1 or 2 to samples with label 3 (i.e., pieces that occur both before and after the change point). This ensures that we have a balanced dataset with labels 1 and 2 representing the digits that occur before and after the change point, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each digit to a label indicating whether it occurs before or after the change point, or both, or neither\n",
    "#  0 - never, 1 - before, 2 - after, 3 - both\n",
    "label_map = {\n",
    "    0: 1,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 2,\n",
    "    4: 0,\n",
    "    5: 0,\n",
    "    6: 0,\n",
    "    7: 3,\n",
    "    8: 0,\n",
    "    9: 3\n",
    "}\n",
    "y_mapped = np.array([label_map[digit] for digit in y])\n",
    "\n",
    "# Remove samples with label 0 (i.e., digits that do not occur before or after the change point) \n",
    "#  from input features and labels\n",
    "X_clean = X.copy().reshape(-1, 28*28)[y_mapped != 0]\n",
    "y_clean = y.copy()[y_mapped != 0]\n",
    "y_mapped_clean = y_mapped.copy()[y_mapped != 0]\n",
    "\n",
    "# Randomly assign labels of 1 or 2 to samples with label 3\n",
    "#  (i.e., digits that occur both before and after the change point)\n",
    "label_3_idx = np.where(y_mapped_clean == 3)[0]\n",
    "y_mixed = y_mapped_clean.copy()\n",
    "y_mixed[label_3_idx] = np.random.choice([1, 2], size=len(label_3_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fce952",
   "metadata": {},
   "source": [
    "## Computing the RF-Kernel Matrix and Visualizing Learned Similarities Between Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7cf19",
   "metadata": {},
   "source": [
    "In this part, we train a random forest classifier on the mixed training set of fasion piece, where the pieces that occur both before and after the distribution shift have been randomly assigned to either the before or after group. We use the trained classifier to compute an RF-kernel-matrix for a subset of the input features and labels. We then display the RF-kernel-matrix, where the samples are grouped by original class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a boolean flag to determine whether to skip model selection and train with max_leaf_nodes=150\n",
    "skip_model_selection = True\n",
    "\n",
    "# Split the dataset into a training set and a test set, with a 55/45 split\n",
    "X_clean_train, X_clean_test, y_mixed_train, y_mixed_test = \\\n",
    "    train_test_split(X_clean.reshape(-1,28*28), y_mixed, train_size=0.55)\n",
    "\n",
    "# Initialize a random forest model with max_leaf_nodes=150\n",
    "model = RandomForestClassifier(min_samples_leaf=500,max_leaf_nodes=15)\n",
    "#model = ExtraTreesClassifier(min_samples_leaf=50,max_leaf_nodes=25)\n",
    "\n",
    "# Train the model on the mixed training set (group 3 randomly assigned to 1 or 2)\n",
    "if not skip_model_selection:\n",
    "    best_model, best_score = None, -5\n",
    "    start_nodes, end_nodes, step_size = 15, 50, 5\n",
    "    for max_leaf_nodes in range(start_nodes, end_nodes+1, step_size):\n",
    "        # Train a random forest model with the current value of max_leaf_nodes\n",
    "        rf_model = RandomForestClassifier(min_samples_leaf=max_leaf_nodes).fit(X_clean_train, y_mixed_train)\n",
    "        # Evaluate the model on the test set and store the score\n",
    "        test_score = rf_model.score(X_clean_test, y_mixed_test)\n",
    "        print(f\"Test set accuracy for max_leaf_nodes = {max_leaf_nodes}: {test_score:.3f}\")\n",
    "        # If the current model has a higher score than the previous best model, update the best model and best score\n",
    "        if test_score > best_score:\n",
    "            best_model = rf_model\n",
    "            best_score = test_score\n",
    "    print(\"----\")\n",
    "    print(f\"Best test set accuracy: {best_score:.3f}\")\n",
    "    # Set the model to the best model found during model selection\n",
    "    model = best_model\n",
    "\n",
    "# Fit the model to the mixed set (group 3 is randomly assigned to 1 or 2)\n",
    "print('Fitting Random Forest classifier...')\n",
    "model.fit(X_clean, y_mixed);\n",
    "print('Fitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of the clean input features and labels\n",
    "subset_size = 5000\n",
    "subset_indices = np.random.choice(range(X_clean.shape[0]), size=subset_size, replace=False)\n",
    "X_subset, y_subset = X_clean[subset_indices], y_clean[subset_indices]\n",
    "\n",
    "# Compute the RF-Kernel-Matrix for the subset using the trained random forest model\n",
    "print('Computing Random Forest kernel matrix...')\n",
    "rf_kernel_matrix = np.zeros((subset_size, subset_size))\n",
    "\n",
    "leaf_indices = model.apply(X_subset.reshape(-1,28*28))\n",
    "for leaf_vector in leaf_indices.T:\n",
    "    # Compute the pairwise similarity between leaf indices using boolean array comparison\n",
    "    rf_kernel_matrix += leaf_vector[:, None] == leaf_vector[None, :]\n",
    "\n",
    "# Normalize the RF-Kernel-Matrix by the number of decision trees in the random forest\n",
    "rf_kernel_matrix /= leaf_indices.shape[1]\n",
    "print('Computing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the subset of input features and labels by their ground truth class\n",
    "sorted_indices_by_class = np.argsort(y_subset)\n",
    "\n",
    "# Display the RF-kernel matrix, with samples grouped by class\n",
    "plt.set_cmap(\"viridis\")\n",
    "plt.matshow(rf_kernel_matrix[sorted_indices_by_class, :][:, sorted_indices_by_class])\n",
    "\n",
    "# Get the unique labels and their counts in the sorted subset\n",
    "unique_labels, label_counts = np.unique(y_subset, return_counts=True)\n",
    "\n",
    "# Sort the unique labels and their counts by label\n",
    "sort_indices = np.argsort(unique_labels)\n",
    "unique_labels = unique_labels[sort_indices]\n",
    "label_counts = label_counts[sort_indices]\n",
    "\n",
    "# Calculate the midpoint of each group of samples\n",
    "midpoints = np.cumsum(label_counts) - label_counts / 2 - 1\n",
    "\n",
    "# Add x and y ticks with labels for each groupplt.xticks(midpoints, unique_labels)\n",
    "plt.xticks(midpoints, [labels[i] for i in unique_labels])\n",
    "plt.xlabel('Fahion Piece Class')\n",
    "plt.yticks(midpoints, [labels[i] for i in unique_labels])\n",
    "plt.ylabel('Fahion Piece Class')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Similarities from RF classifier sorted by pieces')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643a257",
   "metadata": {},
   "source": [
    "This plot visualizes the computed similarity between pairs of samples derived from the classification model. High similarity is encoded by bright colors. The plot shows the samples sorted by their ground truth classes, which are mapped to labels indicating whether they occurred before or after the change point, or both. The original digit classes are unknown to the classifier. We can see that the model is able to distinguish well between the drifiting behaviou and original classes. Note that even though Sneaker and Ankle boot was randomly assigned to either group 1 or 2, the model was able to separate this class from the others very clearly resulting in a single block, which indicates that samples of both clases are comprably similar compared to the other classes with different drift profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f73c83",
   "metadata": {},
   "source": [
    "## t-SNE Visualization of Random Forest Kernel Matrix Embeddings with Grouped Pices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad17302",
   "metadata": {},
   "source": [
    "In the previous section, we computed the similarity matrix of the random forest classifier and plotted it. In this section, we apply t-SNE to the first five principal components of the eigenvectors of the RF-kernel-matrix and visualize the results. We show that the resulting plot exhibits clear cluster structure, corresponding to the different digit types. We also show that the color of each point in the plot can be interpreted as the probability of the point occurring before or after the change point. This plot allows us to visually explore the relationship between the digit types and their temporal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the eigenvalues and eigenvectors of the RF-kernel-matrix\n",
    "print('Computing eigenvalues and eigenvectors...')\n",
    "embedding_eigenvalues, embedding_eigenvectors = eigs(rf_kernel_matrix, k=6, which='LM')\n",
    "print('Computing eigenvalues and eigenvectors complete.')\n",
    "\n",
    "# Apply t-SNE to the first 5 principal components of the eigenvectors of the RF-Kernel-Matrix\n",
    "print('Fitting t-SNE model...')\n",
    "tsne_model = TSNE(init='random', learning_rate='auto')\n",
    "tsne_embedding = tsne_model.fit_transform(np.real((embedding_eigenvectors * (embedding_eigenvalues**0.5)[None,:])))\n",
    "print('Fitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the percentage of samples to display for each label\n",
    "percentage = 30\n",
    "\n",
    "# Set the colormap to 'coolwarm' and plot the samples using different markers and colors for each label\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.set_cmap(\"coolwarm\")\n",
    "for label in set(y_subset):\n",
    "    selection_label = y_subset == label\n",
    "    # Calculate the number of samples to display based on the specified percentage\n",
    "    num_samples = int(np.ceil(np.sum(selection_label) * percentage / 100))\n",
    "    # Select a random subset of the samples with the current label\n",
    "    selection_random = np.random.choice(np.where(selection_label)[0], size=num_samples, replace=False)\n",
    "    if selection_label.sum() > 0:\n",
    "        # Plot the samples with the current label, using the label as the marker and the model's predicted probability \n",
    "        #  as the color\n",
    "        plt.scatter(tsne_embedding[selection_random][:,0], tsne_embedding[selection_random][:,1], marker=[\"o\",\"*\",\"^\",\"v\",\"\",\"\",\"\",\"X\",\"\",\"P\"][label],\n",
    "                    #label=[\"T-Shirt\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"][label],\n",
    "                    c=model.predict_proba(X_subset[selection_random].reshape(-1,28*28))[:,0], alpha=0.5, s=40)\n",
    "\n",
    "# Add legend (use full dataset for better estimation of mean value)\n",
    "for label in range(6):\n",
    "    plt.plot([], [], [\"o\",\"*\",\"^\",\"v\",\"X\",\"P\"][label],\n",
    "            label=labels[[0,1,2,3,7,9][label]],\n",
    "            color=plt.get_cmap()(model.predict_proba(X[y == [0,1,2,3,7,9][label]].reshape(-1,28*28))[:,0].mean()))\n",
    "        \n",
    "# Display the plot\n",
    "plt.legend()\n",
    "plt.xlabel(\"t-SNE dimension 1\")\n",
    "plt.ylabel(\"t-SNE dimension 2\")\n",
    "plt.title(f't-SNE Embedding of MNIST Data Colored by Random Forest Predictions ({percentage}% of the dataset)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1332ba",
   "metadata": {},
   "source": [
    "The t-SNE plot shows a clear clustering structure according to the digit types. Each cluster can be identified in its time context by the color that is computed from the RF prediction. This color coding gives rise to the two time dependent groups. Note that the structure that we obtained by the kernel analysis has two main advantages over solely evaluating the color coded class probability: It helps us distinguish between different kinds of data points that would be indistinguishable when looking at the predicted class probabilities only. Furthermore it helps identify time-independent clusters even if the classifier does not predict each of its samples with 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648d78-60dd-4b31-9ca0-f0ec96a1d298",
   "metadata": {},
   "source": [
    "## Drift Explanations using Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccef3ad-ff94-4c7e-a1e6-7ea88fd499fd",
   "metadata": {},
   "source": [
    "So far, we have considered a global explanatory scheme using dimensionality reduction. This can be helpful when the global structure of the drift is of interest. However, especially for non-experts, a sampling-based explanation may be easier to understand. In what follows, we will demonstrate such explanations using counterfactuals that provide us with the information in which sense we need to modify certain samples to change their temporal assignment. As a result, we can grasp the important features by contrasting the resulting pair of samples.\n",
    "\n",
    "To apply this explanatory scheme, we must first identify particularly relevant sample points. This is done by applying prototype-based clustering algorithms (k-means in this case) to the metric induced by random forest. In addition, we identify the data points that are acutely affected by the drift and require an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492da1a-19d2-4eb2-aaa3-98a22349de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_decision = 0.6\n",
    "\n",
    "# Compute Euclidean embedding of \n",
    "proj = np.real( (embedding_eigenvectors * embedding_eigenvalues**0.5) )\n",
    "\n",
    "# Compute drift localization / drift regions\n",
    "y_pred = model.predict_proba(X_subset)[:,0] \n",
    "has_drift = 2*np.abs(y_pred-0.5) > 1-theta_decision\n",
    "before_drift = y_pred > 0.5\n",
    "regions = (has_drift*(2*before_drift-1)).astype(int)\n",
    "\n",
    "# Cluster projected data to obtain characteristic samples\n",
    "cluster = MiniBatchKMeans(n_clusters=6).fit_predict(proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89bd4e-c4c1-42ab-96b4-548cbae4a3c1",
   "metadata": {},
   "source": [
    "As a quick sanity check we check the alignment of the found prototypes with the fashion pice classes and the (true) temporal classes (before, after, both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937cbd1-5b52-488c-8d1e-89a2c188e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check using true label information (not avaiilable in practice)\n",
    "confusion_mtx_true_label = np.zeros( (np.unique(y_subset).shape[0],np.unique(cluster).shape[0]) )\n",
    "for i,true_label in enumerate(np.unique(y_subset)):\n",
    "    for j,cluster_id in enumerate(np.unique(cluster)):\n",
    "        confusion_mtx_true_label[i,j] = ((y_subset==true_label)*(cluster==cluster_id)).sum() / (cluster==cluster_id).sum()\n",
    "plt.matshow(confusion_mtx_true_label)\n",
    "plt.title(\"Confusion Matrix Prototype vs. True Label\")\n",
    "plt.xlabel(\"Prototype\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.yticks(range(confusion_mtx_true_label.shape[0]), [labels[label] for label in np.unique(y_subset)])\n",
    "plt.show()\n",
    "\n",
    "# Sanity check using drift information\n",
    "confusion_mtx_time_label = np.zeros( (3,np.unique(cluster).shape[0]) )\n",
    "for i,time_label in enumerate(range(1,4)):\n",
    "    for j,cluster_id in enumerate(np.unique(cluster)):\n",
    "        for true_label in [k for k,v in label_map.items() if v == time_label]:\n",
    "            confusion_mtx_time_label[i,j] += ((y_subset==true_label)*(cluster==cluster_id)).sum() \n",
    "        confusion_mtx_time_label[i,j] /= (cluster==cluster_id).sum()\n",
    "plt.matshow(confusion_mtx_time_label)\n",
    "plt.title(\"Confusion Matrix Prototype vs. Timepoint\")\n",
    "plt.xlabel(\"Prototype\")\n",
    "plt.ylabel(\"Timepoint\")\n",
    "plt.yticks([0,1,2],[\"before drift\",\"after drift\",\"both\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3551c60-a238-45f3-8ab9-b191baa7ecb0",
   "metadata": {},
   "source": [
    "We determine the most relevant data points by selecting those that are closest to the obtained prototypes and belong to a particular temporal class as predicted by the model. To measure proximity, we use the Euclidean distance or the distance induced by the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef78e9-26b5-42de-9f91-dfcf0ece7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Characteristic Samples and Counterfactual\n",
    "counterfactuals = []\n",
    "for cluster_id in np.unique(cluster):\n",
    "    cf = dict()\n",
    "    cf[\"region_dist\"] = np.eye(3)[regions[cluster == cluster_id]+1].mean(axis=0)\n",
    "    cf[\"region\"] = [-1,0,1][np.argmax(cf[\"region_dist\"])]\n",
    "    cf[\"sample_euc\"] = dict()\n",
    "    cf[\"sample_rfk\"] = dict()\n",
    "    cf[\"proto_rfk\"] = proj_proto = proj[cluster == cluster_id].mean(axis=0)\n",
    "    cf[\"proto_euc\"] = X_proto = X_subset[cluster == cluster_id].mean(axis=0)\n",
    "    for region in [-1,0,1]:\n",
    "        sel = np.where(regions == region)[0] \n",
    "        cf[\"sample_euc\"][region] = sel[np.argmin( ((X_subset[sel] - X_proto)**2).sum(axis=1) )]\n",
    "        cf[\"sample_rfk\"][region] = sel[np.argmin( ((proj[sel] - proj_proto)**2).sum(axis=1) )]\n",
    "    counterfactuals.append(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7c41d-2882-4862-a123-5313041810df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Characteristic Samples \n",
    "fig = plt.figure(figsize=(5*len(counterfactuals),5))\n",
    "for cf,frames in zip(counterfactuals,list(fig.subplots(2,len(counterfactuals)).T)):\n",
    "    for cf_type,frame in zip([\"sample_euc\",\"sample_rfk\"],list(frames)):\n",
    "        frame.imshow(X_subset[cf[cf_type][cf[\"region\"]]].reshape(28,28))\n",
    "        frame.set_xticks([])\n",
    "        frame.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4893f-969a-4219-8356-eafe69386eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Counterfactuals\n",
    "drifting_counterfactuals = list(filter(lambda cf: cf[\"region\"] != 0, counterfactuals))\n",
    "\n",
    "for cf_type in [\"sample_euc\",\"sample_rfk\"]:\n",
    "    print(cf_type)\n",
    "    fig = plt.figure(figsize=(5*len(drifting_counterfactuals),5))\n",
    "    for cf,frames in zip(drifting_counterfactuals,list(fig.subplots(2,len(drifting_counterfactuals)).T)):\n",
    "        for rel_region,frame in zip([1,-1],list(frames)):\n",
    "            frame.imshow(X_subset[cf[cf_type][rel_region*cf[\"region\"]]].reshape(28,28))\n",
    "            frame.set_xticks([])\n",
    "            frame.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438458ea-e59b-498f-8fdc-cb53002e8472",
   "metadata": {},
   "source": [
    "As can be seen, before the drift we have pullovers and dresses, after the drift we have pants and shirts. This matches the initial setting. Also, as expected, the Euclidean distance tries to fit the shape of the fashion pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281b7a6-6459-49be-b7b8-73a8d979f393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
