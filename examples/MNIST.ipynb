{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7df9670",
   "metadata": {},
   "source": [
    "# Explaining Distribution Shifts in Time using Random Forests and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4441cc5",
   "metadata": {},
   "source": [
    "In this Python notebook, we use machine learning techniques to explain a distribution shift in a time series. Specifically, we aim to identify which data points are specific to the time before or after the distribution shift, as well as which data points are distributed independently of time.\n",
    "\n",
    "To achieve this goal, we train a random forest (RF) model on the MNIST dataset, which contains handwritten digits. MNIST is a classic dataset in machine learning that is often used as a benchmark for image classification tasks, as it presents a challenging high-dimensional classification problem with various classes to choose from. The random forest model is a versatile and robust model that can handle a wide range of input data and is relatively insensitive to weak data assumptions. The model is well-suited to handle the complex and high-dimensional input data in the MNIST dataset, making it a good choice for this machine learning task.\n",
    "\n",
    "To visualize the results, we use t-SNE to project the high-dimensional data onto a two-dimensional plot. The t-SNE algorithm is used to derive an embedding space from the information that is extracted during the random forest training. The plot shows how clusters change before and after the distribution shift, providing insights to better understand the nature of the shift. By visualizing the embedding space, we can see which data points are specific to the time before or after the distribution shift and how the clusters change over time. This information can be used to improve the performance of the random forest model and gain a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f39f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse.linalg import eigs\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6381766",
   "metadata": {},
   "source": [
    "## Exploring the MNIST Dataset: Mean Images, Similarity Matrix, and t-SNE Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dddb73",
   "metadata": {},
   "source": [
    "In this part, we explore the MNIST dataset using mean images, similarity, and t-SNE. We first compute the mean image for each digit class and use it to compute the similarity between each pair of digit classes. We then visualize the similarity matrix using matplotlib. Finally, we use t-SNE to project the high-dimensional data onto a two-dimensional space, which allows us to visualize the dataset and gain insights into its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading the dataset...')\n",
    "mnist = fetch_openml('mnist_784')\n",
    "print('Download complete.')\n",
    "\n",
    "# Store the dataset in an appropriate format\n",
    "X = np.array(mnist.data).reshape((-1, 28, 28))\n",
    "y = np.array(mnist.target).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e4896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty numpy array to store the mean images for each label\n",
    "Xmean = np.zeros((len(set(y)), 28, 28))\n",
    "\n",
    "# Iterate over each label in the dataset\n",
    "for i in set(y):\n",
    "    # Compute the mean image for the current label\n",
    "    Xmean[i] = X[y == i].mean(axis=0)\n",
    "    \n",
    "# Create a figure with two rows and five columns to plot class means\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 3))\n",
    "\n",
    "# Iterate over each subplot and plot the data\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        label = i * 5 + j\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].matshow(Xmean[label])\n",
    "        axs[i, j].set_title('label {} mean:'.format(label))\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90925851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty numpy array to store the dot product between the mean images\n",
    "scalar = np.zeros((Xmean.shape[0], Xmean.shape[0]))\n",
    "\n",
    "# Iterate over each pair of labels and compute the dot product between their mean images\n",
    "for i in range(Xmean.shape[0]):\n",
    "    for j in range(Xmean.shape[0]):\n",
    "        scalar[i, j] = Xmean[i].flatten() @ Xmean[j].flatten()\n",
    "\n",
    "# Normalize the similarity matrix so that its rows and columns sum to 1\n",
    "scalar = scalar / (scalar.sum(axis=0)[:, None] * scalar.sum(axis=1)[None, :])\n",
    "\n",
    "# Display the similarity matrix using matplotlib\n",
    "plt.matshow(scalar)\n",
    "plt.xlabel('Digit Class')\n",
    "plt.xticks(range(10))\n",
    "plt.ylabel('Digit Class')\n",
    "plt.yticks(range(10))\n",
    "plt.title('Similarity matrix between mean images')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51774ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 5000 samples from the dataset\n",
    "embedding_selection = np.random.choice(range(X.shape[0]), size=5000, replace=False)\n",
    "\n",
    "# Fit the t-SNE model to the selected samples and obtain the low-dimensional embeddings\n",
    "print('Fitting the t-SNE model...')\n",
    "tsne = TSNE(init='random', learning_rate='auto')\n",
    "X_embedded = tsne.fit_transform(X[embedding_selection].reshape(-1, 28*28))\n",
    "print('Fitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size to 10x10 inches\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Specify the percentage of samples to display for each label\n",
    "percentage = 30\n",
    "\n",
    "# Iterate over each label in the dataset\n",
    "for i in set(y):\n",
    "    # Select the samples with the current label\n",
    "    selection_label = y[embedding_selection] == i\n",
    "    # Calculate the number of samples to display based on the specified percentage\n",
    "    num_samples = int(np.ceil(np.sum(selection_label) * percentage / 100))\n",
    "    # Select a random subset of the samples with the current label\n",
    "    selection_random = np.random.choice(np.where(selection_label)[0], size=num_samples, replace=False)\n",
    "    # Create a scatter plot of the selected samples, with the label as the marker and alpha set to 0.5\n",
    "    plt.scatter(X_embedded[selection_random, 0], X_embedded[selection_random, 1], marker=\"$%i$\" % i, alpha=0.5, s=25)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(f't-SNE visualization of MNIST ({percentage}% of the dataset)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41b907",
   "metadata": {},
   "source": [
    "## Preparing the data for the distribution shift analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de0179",
   "metadata": {},
   "source": [
    "In this part, we prepare the MNIST dataset for the distribution shift analysis. We map each digit in the dataset to a label indicating whether it occurs before or after the change point, or both, or neither. We then remove the samples with label 0 (i.e., digits that do not occur before or after the change point) from the input features and labels. Finally, we randomly assign labels of 1 or 2 to samples with label 3 (i.e., digits that occur both before and after the change point). This ensures that we have a balanced dataset with labels 1 and 2 representing the digits that occur before and after the change point, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each digit to a label indicating whether it occurs before or after the change point, or both, or neither\n",
    "#  0 - never, 1 - before, 2 - after, 3 - both\n",
    "label_map = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 3,\n",
    "    5: 0,\n",
    "    6: 0,\n",
    "    7: 2,\n",
    "    8: 2,\n",
    "    9: 0\n",
    "}\n",
    "y_mapped = np.array([label_map[digit] for digit in y])\n",
    "\n",
    "# Remove samples with label 0 (i.e., digits that do not occur before or after the change point) \n",
    "#  from input features and labels\n",
    "X_clean = X.copy().reshape(-1, 28*28)[y_mapped != 0]\n",
    "y_clean = y.copy()[y_mapped != 0]\n",
    "y_mapped_clean = y_mapped.copy()[y_mapped != 0]\n",
    "\n",
    "# Randomly assign labels of 1 or 2 to samples with label 3\n",
    "#  (i.e., digits that occur both before and after the change point)\n",
    "label_3_idx = np.where(y_mapped_clean == 3)[0]\n",
    "y_mixed = y_mapped_clean.copy()\n",
    "y_mixed[label_3_idx] = np.random.choice([1, 2], size=len(label_3_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fce952",
   "metadata": {},
   "source": [
    "## Computing the RF-Kernel Matrix and Visualizing Learned Similarities Between Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7cf19",
   "metadata": {},
   "source": [
    "In this part, we train a random forest classifier on the mixed training set of digits, where the digits that occur both before and after the distribution shift have been randomly assigned to either the before or after group. We use the trained classifier to compute an RF-kernel-matrix for a subset of the input features and labels. We then display the RF-kernel-matrix, where the samples are grouped by digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a boolean flag to determine whether to skip model selection and train with max_leaf_nodes=150\n",
    "skip_model_selection = True\n",
    "\n",
    "# Split the dataset into a training set and a test set, with a 55/45 split\n",
    "X_clean_train, X_clean_test, y_mixed_train, y_mixed_test = \\\n",
    "    train_test_split(X_clean.reshape(-1,28*28), y_mixed, train_size=0.55)\n",
    "\n",
    "# Initialize a random forest model with max_leaf_nodes=150\n",
    "model = RandomForestClassifier(max_leaf_nodes=150)\n",
    "\n",
    "# Train the model on the mixed training set (group 3 randomly assigned to 1 or 2)\n",
    "if not skip_model_selection:\n",
    "    best_model, best_score = None, -5\n",
    "    start_nodes, end_nodes, step_size = 50, 200, 5\n",
    "    for max_leaf_nodes in range(start_nodes, end_nodes+1, step_size):\n",
    "        # Train a random forest model with the current value of max_leaf_nodes\n",
    "        rf_model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes).fit(X_clean_train, y_mixed_train)\n",
    "        # Evaluate the model on the test set and store the score\n",
    "        test_score = rf_model.score(X_clean_test, y_mixed_test)\n",
    "        print(f\"Test set accuracy for max_leaf_nodes = {max_leaf_nodes}: {test_score:.3f}\")\n",
    "        # If the current model has a higher score than the previous best model, update the best model and best score\n",
    "        if test_score > best_score:\n",
    "            best_model = rf_model\n",
    "            best_score = test_score\n",
    "    print(\"----\")\n",
    "    print(f\"Best test set accuracy: {best_score:.3f}\")\n",
    "    # Set the model to the best model found during model selection\n",
    "    model = best_model\n",
    "\n",
    "# Fit the model to the mixed set (group 3 is randomly assigned to 1 or 2)\n",
    "print('Fitting Random Forest classifier...')\n",
    "model.fit(X_clean, y_mixed);\n",
    "print('Fitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of the clean input features and labels\n",
    "subset_size = 5000\n",
    "subset_indices = np.random.choice(range(X_clean.shape[0]), size=subset_size, replace=False)\n",
    "X_subset, y_subset = X_clean[subset_indices], y_clean[subset_indices]\n",
    "\n",
    "# Compute the RF-Kernel-Matrix for the subset using the trained random forest model\n",
    "print('Computing Random Forest kernel matrix...')\n",
    "rf_kernel_matrix = np.zeros((subset_size, subset_size))\n",
    "\n",
    "leaf_indices = model.apply(X_subset.reshape(-1,28*28))\n",
    "for leaf_vector in leaf_indices.T:\n",
    "    # Compute the pairwise similarity between leaf indices using boolean array comparison\n",
    "    rf_kernel_matrix += leaf_vector[:, None] == leaf_vector[None, :]\n",
    "\n",
    "# Normalize the RF-Kernel-Matrix by the number of decision trees in the random forest\n",
    "rf_kernel_matrix /= leaf_indices.shape[1]\n",
    "print('Computing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the subset of input features and labels by their ground truth class\n",
    "sorted_indices_by_class = np.argsort(y_subset)\n",
    "\n",
    "# Display the RF-kernel matrix, with samples grouped by class\n",
    "plt.set_cmap(\"viridis\")\n",
    "plt.matshow(rf_kernel_matrix[sorted_indices_by_class, :][:, sorted_indices_by_class])\n",
    "\n",
    "# Get the unique labels and their counts in the sorted subset\n",
    "unique_labels, label_counts = np.unique(y_subset, return_counts=True)\n",
    "\n",
    "# Sort the unique labels and their counts by label\n",
    "sort_indices = np.argsort(unique_labels)\n",
    "unique_labels = unique_labels[sort_indices]\n",
    "label_counts = label_counts[sort_indices]\n",
    "\n",
    "# Calculate the midpoint of each group of samples\n",
    "midpoints = np.cumsum(label_counts) - label_counts / 2 - 1\n",
    "\n",
    "# Add x and y ticks with labels for each groupplt.xticks(midpoints, unique_labels)\n",
    "plt.xticks(midpoints, unique_labels)\n",
    "plt.xlabel('Digit Class')\n",
    "plt.yticks(midpoints, unique_labels)\n",
    "plt.ylabel('Digit Class')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Similarities from RF classifier sorted by digits')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643a257",
   "metadata": {},
   "source": [
    "This plot visualizes the computed similarity between pairs of samples derived from the classification model. High similarity is encoded by bright colors. The plot shows the samples sorted by their ground truth digit classes 1, 3, 4, 7, and 8, which were mapped to labels indicating whether they occurred before or after the change point, or both. The original digit classes are unknown to the classifier. We can see that the model is able to distinguish well between the two main groups, formed by digits 1 and 3 (group 1) and digits 7 and 8 (group 2). Note that even though digit 4 was randomly assigned to either group 1 or 2, the model was able to separate this class from the others very clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f73c83",
   "metadata": {},
   "source": [
    "## t-SNE Visualization of Random Forest Kernel Matrix Embeddings with Grouped Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad17302",
   "metadata": {},
   "source": [
    "In the previous section, we computed the similarity matrix of the random forest classifier and plotted it. In this section, we apply t-SNE to the first five principal components of the eigenvectors of the RF-kernel-matrix and visualize the results. We show that the resulting plot exhibits clear cluster structure, corresponding to the different digit types. We also show that the color of each point in the plot can be interpreted as the probability of the point occurring before or after the change point. This plot allows us to visually explore the relationship between the digit types and their temporal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the eigenvalues and eigenvectors of the RF-kernel-matrix\n",
    "print('Computing eigenvalues and eigenvectors...')\n",
    "embedding_eigenvalues, embedding_eigenvectors = eigs(rf_kernel_matrix, k=5, which='LM')\n",
    "print('Computing eigenvalues and eigenvectors complete.')\n",
    "\n",
    "# Apply t-SNE to the first 5 principal components of the eigenvectors of the RF-Kernel-Matrix\n",
    "print('Fitting t-SNE model...')\n",
    "tsne_model = TSNE(init='random', learning_rate='auto')\n",
    "tsne_embedding = tsne_model.fit_transform(np.real((embedding_eigenvectors * (embedding_eigenvalues**0.5)[None,:])))\n",
    "print('Fitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the percentage of samples to display for each label\n",
    "percentage = 30\n",
    "\n",
    "# Set the colormap to 'coolwarm' and plot the samples using different markers and colors for each label\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.set_cmap(\"coolwarm\")\n",
    "for label in set(y_subset):\n",
    "    selection_label = y_subset == label\n",
    "    # Calculate the number of samples to display based on the specified percentage\n",
    "    num_samples = int(np.ceil(np.sum(selection_label) * percentage / 100))\n",
    "    # Select a random subset of the samples with the current label\n",
    "    selection_random = np.random.choice(np.where(selection_label)[0], size=num_samples, replace=False)\n",
    "    if selection_label.sum() > 0:\n",
    "        # Plot the samples with the current label, using the label as the marker and the model's predicted probability \n",
    "        #  as the color\n",
    "        plt.scatter(tsne_embedding[selection_random][:,0], tsne_embedding[selection_random][:,1], marker=\"$%i$\" % label, \n",
    "                    c=model.predict_proba(X_subset[selection_random].reshape(-1,28*28))[:,0], alpha=0.5, s=40)\n",
    "\n",
    "# Display the plot\n",
    "plt.xlabel(\"t-SNE dimension 1\")\n",
    "plt.ylabel(\"t-SNE dimension 2\")\n",
    "plt.title(f't-SNE Embedding of MNIST Data Colored by Random Forest Predictions ({percentage}% of the dataset)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1332ba",
   "metadata": {},
   "source": [
    "The t-SNE plot shows a clear clustering structure according to the digit types. Each cluster can be identified in its time context by the color that is computed from the RF prediction. This color coding gives rise to the two time dependent groups formed by digits 1 and 3 (group 1) and digits 7 and 8 (group 2). Each of the five digits, in particular the randomly assigned digit 4, is well distinguishable in the clustering structure. Note that the structure that we obtained by the kernel analysis has two main advantages over solely evaluating the color coded class probability: It helps us distinguish between different kinds of data points that would be indistinguishable when looking at the predicted class probabilities only. Furthermore it helps identify time-independent clusters even if the classifier does not predict each of its samples exactly with 0.5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
